"""
Basic layer definitions for applied batch normalization.
"""
from tensorflow.keras.layers import Dense, BatchNormalization, Activation, Conv1D
from tensorflow.keras import backend as K
from tensorflow.keras.regularizers import Regularizer

import numpy as np


class OrthogonalRegularizer(Regularizer):
    """
    Considering that input is flattened square matrix X, regularizer tries to ensure that matrix X
    is orthogonal, i.e. ||X*X^T - I|| = 0. L1 and L2 penalties can be applied to it
    """
    def __init__(self, l1=0.0, l2=0.0):
        self.l1 = l1
        self.l2 = l2

    def __call__(self, x):
        size = int(np.sqrt(x.shape[1]))
        assert size * size == x.shape[1]
        x = K.reshape(x, (-1, size, size))
        xxt = K.batch_dot(x, x, axes=(2, 2))
        regularization = 0.0
        if self.l1:
            regularization += K.sum(self.l1 * K.abs(xxt - K.eye(size)))
        if self.l2:
            regularization += K.sum(self.l2 * K.square(xxt - K.eye(size)))

        return regularization

    def get_config(self):
        return {'l1': float(self.l1), 'l2': float(self.l2)}


def dense_bn(x, units, use_bias=True, scope=None, activation=None):
    """
    Utility function to apply Dense + Batch Normalization.
    """
    with K.name_scope(scope):
        x = Dense(units=units, use_bias=use_bias)(x)
        x = BatchNormalization(momentum=0.9)(x)
        x = Activation(activation)(x)
    return x


def conv1d_bn(x, num_filters, kernel_size, padding='same', strides=1,
              use_bias=False, scope=None, activation='relu'):
    """
    Utility function to apply Convolution + Batch Normalization.
    """
    #with K.name_scope(scope):
    input_shape = x.get_shape().as_list()[-2:]
    x = Conv1D(num_filters, kernel_size, strides=strides, padding=padding,
               use_bias=use_bias, input_shape=input_shape)(x)
    x = BatchNormalization(momentum=0.9)(x)
    x = Activation(activation)(x)
    return x